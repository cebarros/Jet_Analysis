{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a70535a-340d-4e6c-a046-e4fbcb2912a0",
   "metadata": {},
   "source": [
    "# QCD Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f741e12-8a10-411c-9218-29a5d6d0c88c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "import numpy as np\n",
    "import coffea\n",
    "import uproot\n",
    "import hist\n",
    "import vector\n",
    "from coffea import util, processor\n",
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema, BaseSchema\n",
    "from distributed.diagnostics.plugin import UploadDirectory\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import pickle\n",
    "import correctionlib\n",
    "from coffea.analysis_tools import PackedSelection\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4a1638-12dc-4881-b5f5-ba7f7e457984",
   "metadata": {},
   "source": [
    "To start with, we define the only correction function we will use on our data, which will provide a correction to the pileup scale factor. We will use the json files in `data/pu_weights` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4756c581-b4e1-41fa-87b6-9ed6df836445",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GetPUSF(IOV, nTrueInt, var='nominal'):\n",
    "    \n",
    "    corrlib_namemap = {\n",
    "    \"2016APV\":\"2016preVFP_UL\",\n",
    "    \"2016\":\"2016postVFP_UL\",\n",
    "    \"2017\":\"2017_UL\",\n",
    "    \"2018\":\"2018_UL\"\n",
    "    }\n",
    "    \n",
    "    fname = \"data/pu_weights/\" + corrlib_namemap[IOV] + \"/puWeights.json.gz\"\n",
    "    hname = {\n",
    "        \"2016APV\": \"Collisions16_UltraLegacy_goldenJSON\",\n",
    "        \"2016\"   : \"Collisions16_UltraLegacy_goldenJSON\",\n",
    "        \"2017\"   : \"Collisions17_UltraLegacy_goldenJSON\",\n",
    "        \"2018\"   : \"Collisions18_UltraLegacy_goldenJSON\"\n",
    "    }\n",
    "    \n",
    "    evaluator = correctionlib.CorrectionSet.from_file(fname)\n",
    "    return evaluator[hname[IOV]].evaluate(np.array(nTrueInt), var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3936f0bc-f5b5-4494-921b-ee3fcaed24a5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed44b94e-a4c1-4a4f-a6bf-32fe54d16a4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QCDProcessor(processor.ProcessorABC):\n",
    "        \n",
    "    def __init__(self):\n",
    "        \n",
    "        ############################################\n",
    "        ### Defining the axes for the histograms ###\n",
    "        ############################################\n",
    "        \n",
    "        dataset_axis = hist.axis.StrCategory([], growth=True, name=\"dataset\", label=\"Primary dataset\")\n",
    "        frac_axis = hist.axis.Regular(300, 0, 2.0, name=\"frac\", label=r\"Fraction\")               \n",
    "        #pt_axis = hist.axis.Variable([10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 150, 200, 250, 300, 350, 400, 450, 500,\n",
    "        #                            550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 6000, 7000, 8000, 9000, 10000], name=\"pt\", label=r\"$p_{T}$ [GeV]\")\n",
    "        \n",
    "        pt_axis = hist.axis.Variable([10 ,  11  , 12 ,  13  , 14  , 15 ,  17, 20  , 23  , 27   ,30  , 35   ,40  , 45 ,  57 ,  72  , 90  , 120 ,  150, 200  , 300 ,  400   ,550 ,  750 ,  1000 ,\n",
    "                                    1500  , 2000 ,  2500  , 3000, 3500 ,  4000  ,  5000   ,10000], name=\"pt\", label=r\"$p_{T}$ [GeV]\")\n",
    "        \n",
    "        pileup_axis = hist.axis.Variable([0, 10, 20, 30, 40, 50, 60, 70, 80], name=\"pileup\", label=r\"$\\mu$\")\n",
    "        pileup_fine_axis = hist.axis.Regular(30, 0, 40, name=\"pileup_fine\", label=r\"$\\mu$\")\n",
    "        rho_fine_axis = hist.axis.Regular(30, 0, 30, name=\"rho_fine\", label=r\"$\\rho$\")\n",
    "        eta_axis = hist.axis.Variable([0, 0.261, 0.522, 0.783, 1.044, 1.305, 1.566, 1.74, 1.93, 2.043, 2.172, 2.322, 2.5, 2.65, 2.853, 2.964, 3.139, 3.489, 3.839, 5.191], name=\"eta\", label=r\"$\\eta$\")\n",
    "        \n",
    "        ######################################\n",
    "        ### Defining the histogram objects ###\n",
    "        ######################################\n",
    "             \n",
    "        h_pt_reco_over_gen = hist.Hist(dataset_axis, pt_axis, frac_axis, eta_axis, pileup_axis, storage=\"weight\", label=\"Counts\")\n",
    "        h_pt_reco_over_gen_noweight = hist.Hist(dataset_axis, pt_axis, frac_axis, eta_axis, pileup_axis, storage=\"weight\", label=\"Counts\")\n",
    "        h_pileup_rho = hist.Hist(dataset_axis, pileup_fine_axis, rho_fine_axis, storage=\"weight\", label=\"Counts\")\n",
    "        h_pileup_rho_noweight = hist.Hist(dataset_axis, pileup_fine_axis, rho_fine_axis, storage=\"weight\", label=\"Counts\")\n",
    "                                         \n",
    "        cutflow = {}\n",
    "        \n",
    "        self.hists = {\n",
    "            \"pt_reco_over_gen\":h_pt_reco_over_gen,\n",
    "            \"pt_reco_over_gen_noweight\":h_pt_reco_over_gen_noweight,\n",
    "            \"pileup_rho\":h_pileup_rho,\n",
    "            \"pileup_rho_noweight\":h_pileup_rho_noweight,\n",
    "            \"cutflow\":cutflow,\n",
    "        }\n",
    "        \n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self.hists\n",
    "    \n",
    "    def process(self, events):\n",
    "        \n",
    "        dataset = events.metadata['dataset']\n",
    "        print(f\"Processing ----- {dataset}\")\n",
    "        if dataset not in self.hists[\"cutflow\"]:\n",
    "            self.hists[\"cutflow\"][dataset] = defaultdict(int)\n",
    "        \n",
    "        #################################\n",
    "        ### Getting gen and reco jets ###\n",
    "        #################################\n",
    "        \n",
    "        gen_vtx = events.GenVtx.z\n",
    "        reco_vtx = events.PV.z\n",
    "        \n",
    "        events = events[np.abs(gen_vtx - reco_vtx) < 0.2]\n",
    "        \n",
    "        ### Apply jetId mask\n",
    "        \n",
    "        events.Jet = events.Jet[events.Jet.jetId > 0]\n",
    "        events = events[ak.num(events.Jet) > 0]\n",
    "        \n",
    "        ### Match generated jets (events.GenJet) to reconstructed jets (events.Jet) \n",
    "        \n",
    "        genjets = events.GenJet[:,0:3]\n",
    "        recojets = genjets.nearest(events.Jet, threshold=0.2)\n",
    "        \n",
    "        sel = ~ak.is_none(recojets, axis=1)\n",
    "        \n",
    "        genjets = genjets[sel]\n",
    "        recojets = recojets[sel]    \n",
    "        ptresponse = recojets.pt / genjets.pt\n",
    "        \n",
    "        ##################################################################################################\n",
    "        ### Getting the number of primary vertices, number of pileups, and 3D distance to origin (rho) ###\n",
    "        ##################################################################################################\n",
    "        \n",
    "        n_reco_vtx = events.PV.npvs\n",
    "        n_pileup = events.Pileup.nPU\n",
    "        rho = events.fixedGridRhoFastjetAll\n",
    "        pu_nTrueInt = events.Pileup.nTrueInt\n",
    "        \n",
    "        sel = ~ak.is_none(ptresponse, axis=1)\n",
    "        \n",
    "        genjets = genjets[sel]\n",
    "        recojets = recojets[sel]\n",
    "        ptresponse = ptresponse[sel]\n",
    "        \n",
    "        sel2 = ak.num(ptresponse) > 2\n",
    "        \n",
    "        genjets = genjets[sel2]\n",
    "        recojets = recojets[sel2]\n",
    "        ptresponse = ptresponse[sel2]\n",
    "        \n",
    "        n_reco_vtx = n_reco_vtx[sel2]\n",
    "        n_pileup = n_pileup[sel2]\n",
    "        rho = rho[sel2]\n",
    "        pu_nTrueInt = pu_nTrueInt[sel2]\n",
    "        \n",
    "        ##############################################################################\n",
    "        ### Broadcast across recojets, include pileup weights, and fill histograms ###\n",
    "        ##############################################################################\n",
    "        \n",
    "        n_reco_vtx = ak.broadcast_arrays(n_reco_vtx, recojets.pt)[0]\n",
    "        n_pileup = ak.broadcast_arrays(n_pileup, recojets.pt)[0]\n",
    "        rho = ak.broadcast_arrays(rho, recojets.pt)[0]\n",
    "        pu_nTrueInt = ak.broadcast_arrays(pu_nTrueInt, recojets.pt)[0]\n",
    "        \n",
    "        puWeight = GetPUSF(dataset, np.array(ak.flatten(pu_nTrueInt)))\n",
    "        \n",
    "        self.hists[\"pt_reco_over_gen\"].fill(dataset=dataset, pt=ak.flatten(genjets.pt), frac=ak.flatten(ptresponse), \n",
    "                                            eta=np.abs(ak.flatten(genjets.eta)), pileup=ak.flatten(n_pileup), weight=puWeight)\n",
    "        \n",
    "        self.hists[\"pt_reco_over_gen_noweight\"].fill(dataset=dataset, pt=ak.flatten(genjets.pt), frac=ak.flatten(ptresponse), \n",
    "                                            eta=np.abs(ak.flatten(genjets.eta)), pileup=ak.flatten(n_pileup))\n",
    "        \n",
    "        self.hists[\"pileup_rho\"].fill(dataset=dataset, rho_fine=ak.flatten(rho), pileup_fine=ak.flatten(n_pileup), weight=puWeight)\n",
    "\n",
    "        self.hists[\"pileup_rho_noweight\"].fill(dataset=dataset, rho_fine=ak.flatten(rho), pileup_fine=ak.flatten(n_pileup))\n",
    "                                        \n",
    "        return self.hists\n",
    "    \n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1807b108-651d-489f-a48d-6e90982dc51f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "151dabfd-a02f-4fa8-9adb-195c3e0729c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11de2a8d17f04a7896b1428a032c7392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b58d07d03d44e8c839d1ad61e9fff45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ----- 2018Processing ----- 2018\n",
      "\n",
      "Processing ----- 2018\n",
      "Processing ----- 2018\n",
      "Processing ----- 2018\n",
      "Processing ----- 2018\n",
      "Processing ----- 2018\n",
      "Processing ----- 2018\n",
      "Processing ----- 2018\n",
      "Processing ----- 2018\n",
      "Processing ----- 2017\n",
      "Processing ----- 2017\n",
      "Processing ----- 2017\n",
      "Processing ----- 2017\n",
      "Processing ----- 2017\n",
      "Processing ----- 2017\n",
      "Processing ----- 2017\n",
      "Processing ----- 2017\n",
      "Processing ----- 2017\n",
      "Processing ----- 2017\n",
      "Processing ----- 2016APV\n",
      "Processing ----- 2016APV\n",
      "Processing ----- 2016APV\n",
      "Processing ----- 2016APV\n",
      "Processing ----- 2016APV\n",
      "Processing ----- 2016APV\n",
      "Processing ----- 2016APV\n",
      "Processing ----- 2016APV\n",
      "Processing ----- 2016APV\n",
      "Processing ----- 2016\n",
      "Processing ----- 2016\n",
      "Processing ----- 2016\n",
      "Processing ----- 2016\n",
      "Processing ----- 2016\n",
      "Processing ----- 2016Processing ----- 2016\n",
      "\n",
      "Processing ----- 2016\n",
      "Processing ----- 2016\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#prependstr = \"root://cmsxrootd.fnal.gov/\"\n",
    "prependstr = \"root://xcache/\"\n",
    "\n",
    "filedir = \"samples/\"\n",
    "\n",
    "filestr = \"flatPU_JMENano_%s.txt\"\n",
    "\n",
    "eras = [\n",
    "     '2016',\n",
    "     '2016APV',\n",
    "     '2017',\n",
    "     '2018',\n",
    "]\n",
    "\n",
    "fileset = {}\n",
    "\n",
    "for era in eras:\n",
    "    filename = filedir + filestr % (era)\n",
    "    with open(filename) as f:\n",
    "        files = [prependstr + i.rstrip() for i in f.readlines() if i[0] != \"#\"]\n",
    "        fileset[era] = files\n",
    "        \n",
    "futures_run = processor.Runner(\n",
    "    executor = processor.FuturesExecutor(compression=None, workers=2),\n",
    "    schema=NanoAODSchema,\n",
    "    maxchunks=10,\n",
    "    skipbadfiles=True,\n",
    ")\n",
    "\n",
    "out = futures_run(\n",
    "    fileset,\n",
    "    \"Events\",\n",
    "    processor_instance=QCDProcessor()\n",
    ")\n",
    "\n",
    "fname_out = \"pkl_files/QCD_pt_response_NEW.pkl\"\n",
    "\n",
    "with open(fname_out, \"wb\") as f:\n",
    "    pickle.dump(out, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb5fe7a-8991-4b20-82e1-9f9b90c7e6bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
